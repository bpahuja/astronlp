---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- dense
- generated_from_trainer
- dataset_size:34176
- loss:MultipleNegativesRankingLoss
base_model: adsabs/astroBERT
widget:
- source_sentence: This causes the damping of the [MASK] waves with [MATH_tex=mneq
    0] . In Section 4 we showed that the solutions obtained with the Frobenius method
    consistently revert to the approximations found in previous works when the TTTB
    limit is considered .
  sentences:
  - These distances , and the parameters used to derive them , are listed in Table
    2 . [MATH_tex=V] and [MATH_tex=E(B-V)] are derived from the Dunlap Observatory
    Database of [MASK] Classical Cepheids ( 26 ) , [MATH_tex=K] magnitudes from van
    Leeuwen et al .
  - Using a Godunov-MHD code modified to evolve the CGL-MHD equations ( see [ Kowal
    \etal <CIT> , Kowal \etal 2010 ] ) , we performed 3D numerical simulations of
    forced turbulence , starting with a weak seed magnetic field .
  - The telescope is equipped with a 349-pixel photomultiplier tube-based imaging
    camera with a field of view of 6 [MATH_tex=^0] [MATH_tex=times] 6 [MATH_tex=^0]
    and uniform pixel resolution of [MATH_tex=sim] 0.31 [MATH_tex=^0] .
- source_sentence: This ratio , for three coordinates whose [MATH_tex=alpha] and [MATH_tex=beta]
    are given in the Table 2 , is calculated at its average value to 0.46 . In this
    work , the imaging and spectroscopic observations of the central part ( [MATH_tex=13^^primetimes
    13^^prime] ) of the galactic supernova remnant G85.9-0.6 is observed for the first
    time in optical band .
  sentences:
  - The SED of a new RCB can differ from that of a known RCB due to changes in luminosity
    , distance and extinction , where the change in extinction can either be due to
    changes in either Galactic or circumstellar extinction .
  - We apply no correction for possible dust extinction to the colors of SN Refsdal
    , since the low signal-to-noise ratio ( S/N ) or spectral resolution of the spectra
    do not allow any constraint on absorption by , for example , Na I D or diffuse
    interstellar bands . Figure 7 plots the binned WFC3 grism spectra of SN Refsdal
    taken in both the 111 [MATH_tex=^circ] and 119 [MATH_tex=^circ] telescope orientations
    .
  - 'The sample of SNe II employed in this study was obtained by the Carnegie Supernova
    Project ( CSP , Hamuy et al . 11 ) between 2004 and 2009 plus previous campaings
    : the Calan/Tololo Supernova Survey ( CT ) , the Cerro Tololo SN program , the
    Supernova Optical and Infrared Survey ( SOIRS ) and the Carnegie Type II Supernova
    Survey ( CATS ) .'
- source_sentence: The array , completed in the spring of 2007 , is sensitive to a
    point source of 1 % of the steady Crab Nebula flux above [MATH_tex=300 textrmGeV]
    at [MATH_tex=5sigma] in less than 50 hours at 20Ã‚Â°Ã‚ zenith angle . Observations
    are taken in wobble mode <CIT> , with the [MASK] pointing offset by 0.5Ã‚Â°-0.7Ã‚Â°Ã‚
    from the target direction for roughly equal amounts of time in each of four equally
    spaced directions . The data analysis is described more fully in <CIT> .
  sentences:
  - We have obtained 3rd and 4th epoch images in 2007 for the light echo groups we
    detected in 2006 , and the light echoes were redetected in these images . The
    light echo equation ( ) relates the depth coordinate , [MATH_tex=z] , the echo-supernova
    distance projected along the line-of-sight , to the echo distance [MATH_tex=rho]
    perpendicular to the line of sight , and the time [MATH_tex=t] since the explosion
    was observed .
  - We have used the Thermo Scientificâ„¢ LTQ XLâ„¢ linear ion trap ( LTQ ion trap ) as
    described in <CIT> , which is available at the VUV beamline DESIRS at the synchrotron
    SOLEIL \citep nahon2012 . The production of PAH cations in the LTQ ion trap was
    performed using an atmospheric pressure photoionization ( APPI ) source which
    required the species of interest to be in solution before their injection with
    a syringe .
  - Here we apply this technique to the measurement of the redshift-space distortion
    parameter , which in turn improves the measurement of the velocity divergence
    [MASK] [MATH_tex=P_thetathetaequiv f^2P_m(k)] .
- source_sentence: The backreaction parameter has also been estimated in the framework
    of Newtonian [MASK] <CIT> ; it was found that the backreaction term can be quantitatively
    small ( e.g. , [MATH_tex=Omega_cal Q=0.01] ) , but the dynamical influence of
    a nonâ€“vanishing backreaction , [MATH_tex=Omega_I=Omega_k+Omega_cal Q] , on the
    other [MASK] parameters can , in principle , be quite large . Let us discuss the
    effect of averaging on null geodesics in inhomogeneous models .
  sentences:
  - Thus , there is no the Sachs-Wolf effect ( ( 2 ) ) and the expression ( 0.50 )
    is reducible to Unfortunately , well-known software packages such as CAMB ( (
    32 ) ) and CMBFAST ( ( 33 ) ) are absolutely useless for the calculation of CMB
    spectrum in the linear cosmology because they assume a quite different formation
    mechanism for the CMB spectrum peaks .
  - The mid-IR observation was performed as a 24 Micron Survey of the Inner [MASK]
    Disk Program <CIT> . We revisited all the available XMM-Newton data of G57.2 [MATH_tex=+]
    0.8 to search for its extended X-ray emission .
  - The Swift X - ray Telescope observed the X - ray afterglow with a photon spectral
    index of [ MATH _ tex = 1. 98 ^ + 0. 15 _ - 0. 14 ] and absorption column of [
    MATH _ tex = 9. 8 ^ + 3. 3 _ - 3. 1times 10 ^ 20 ] cm [ MATH _ tex = ^ - 2 ] (
    16 ). Swift could not immediately slew to the burst due to Earth - limb constraints,
    but an optical transient ( OT ) was quickly identified using the robotic 2 - m
    Liverpool Telescope ( 42 ). Swift Ultraviolet and Optical Telescope ( UVOT, Roming
    et al. 40 ) observations, beginning [ MATH _ tex = sim 50 ] min post - burst,
    confirmed the existence of this OT at RA [ MATH _ tex = _ rm J2000 ] = [ MATH
    _ tex = 02 ^
- source_sentence: SPH methods for simulating two-fluid mixtures were first developed
    by , improved ( via an implicit treatment of the drag terms ) in and applied in
    an astrophysical context to the dynamics of dust grains in protoplanetary disks
    ( ) .
  sentences:
  - 'While it is possible to use alternative kernels , most modern SPH implementations
    ( including gadget ) utilize a cubic spline kernel ( ) : where [MATH_tex=r] is
    the radius and [MATH_tex=h_rm s] is the characteristic width of the kernel , otherwise
    known as the smoothing length .'
  - For more details , one can refer to Appendix A.4 . We evolve the simulation in
    the concordance model of a LCDM universe specified by the [MASK] parameters [MATH_tex=(Omega_m,Omega_Lambda,h,sigma_8,Omega_b,n_s,z_re)=(0.274,0.72%
  - For example the abundance distributions derived using our radiation transport
    code for type Ia supernovae \citep Ashall18a yields very similar densities and
    abundance distributions to those produced from non-local thermodynamic equilibrium
    radiation hydrodynamical models \citep Hoeflich17 . Our code requires as input
    a density distribution [MATH_tex=rho(r)] ( radius and velocity are equivalent
    in a SN â€™ s homologously expanding ejecta , only a reference time for the density
    is required ) , a Luminosity [MATH_tex=L_bol] , a photospheric velocity [MATH_tex=v_ph]
    , and a set of abundances .
pipeline_tag: sentence-similarity
library_name: sentence-transformers
---

# SentenceTransformer based on adsabs/astroBERT

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [adsabs/astroBERT](https://huggingface.co/adsabs/astroBERT). It maps sentences & paragraphs to a 768-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [adsabs/astroBERT](https://huggingface.co/adsabs/astroBERT) <!-- at revision b692b96fe7c386c096646f52ff2d9b52f995952e -->
- **Maximum Sequence Length:** 256 tokens
- **Output Dimensionality:** 768 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ðŸ¤— Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    'SPH methods for simulating two-fluid mixtures were first developed by , improved ( via an implicit treatment of the drag terms ) in and applied in an astrophysical context to the dynamics of dust grains in protoplanetary disks ( ) .',
    'While it is possible to use alternative kernels , most modern SPH implementations ( including gadget ) utilize a cubic spline kernel ( ) : where [MATH_tex=r] is the radius and [MATH_tex=h_rm s] is the characteristic width of the kernel , otherwise known as the smoothing length .',
    'For example the abundance distributions derived using our radiation transport code for type Ia supernovae \\citep Ashall18a yields very similar densities and abundance distributions to those produced from non-local thermodynamic equilibrium radiation hydrodynamical models \\citep Hoeflich17 . Our code requires as input a density distribution [MATH_tex=rho(r)] ( radius and velocity are equivalent in a SN â€™ s homologously expanding ejecta , only a reference time for the density is required ) , a Luminosity [MATH_tex=L_bol] , a photospheric velocity [MATH_tex=v_ph] , and a set of abundances .',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 768]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# tensor([[1.0000, 0.9843, 0.6336],
#         [0.9843, 1.0000, 0.6607],
#         [0.6336, 0.6607, 1.0000]])
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset

* Size: 34,176 training samples
* Columns: <code>sentence_0</code> and <code>sentence_1</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                          | sentence_1                                                                          |
  |:--------|:------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------|
  | type    | string                                                                              | string                                                                              |
  | details | <ul><li>min: 32 tokens</li><li>mean: 86.52 tokens</li><li>max: 182 tokens</li></ul> | <ul><li>min: 32 tokens</li><li>mean: 85.23 tokens</li><li>max: 182 tokens</li></ul> |
* Samples:
  | sentence_0                                                                                                                                                                                                                                              | sentence_1                                                                                                                                                                                                                                                     |
  |:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | <code>A more detailed report on polarimetry with GREGOR is given by Hofmann <CIT> . M3 reflects the light beam to flat mirror M4 which is located at the intersection of the optical axis of the [MASK] and the elevation axis .</code>                 | <code>We assumed a left-handed system with positive [MATH_tex=U] towards the Galactic anticenter , [MATH_tex=V] in the direction of Galactic rotation , and positive [MATH_tex=W] in the direction to the Galactic north pole .</code>                         |
  | <code>This is not necessarily the optimum choice of weights when including non-Gaussian contributions ( which as shown in Section 4 are significant for the momentum power spectrum ) , nor when optimising cosmological parameter constraints .</code> | <code>For numerical purposes , we make use of the latest version of the publicly available [MASK] code [MASK] ( ( 55 ) ; ( 56 ) ) package which supports the new 2018 Planck likelihood ( ( 1 ) ) and that has been modified to include IDE scenarios .</code> |
  | <code>The telescope is located at the NSF Amundsen-Scott South Pole station , one of the best developed sites on Earth for mm-wave observations , with particularly low levels of atmospheric fluctuation power on degree angular scales <CIT> .</code> | <code>To remove stellar contaminants ( [MASK] M , L and T dwarf stars ) and reduce the sample to secure extragalactic sources , we reduce our high- [MATH_tex=z] dropouts candidates to the extended sources .</code>                                          |
* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:
  ```json
  {
      "scale": 20.0,
      "similarity_fct": "cos_sim",
      "gather_across_devices": false
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `per_device_train_batch_size`: 128
- `per_device_eval_batch_size`: 128
- `num_train_epochs`: 6
- `fp16`: True
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 128
- `per_device_eval_batch_size`: 128
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 6
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: True
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `tp_size`: 0
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: None
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: False
- `prompts`: None
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin
- `router_mapping`: {}
- `learning_rate_mapping`: {}

</details>

### Training Logs
| Epoch  | Step | Training Loss |
|:------:|:----:|:-------------:|
| 1.8727 | 500  | 4.2378        |
| 3.7453 | 1000 | 3.8538        |
| 5.6180 | 1500 | 3.7867        |


### Framework Versions
- Python: 3.12.3
- Sentence Transformers: 5.1.0
- Transformers: 4.51.3
- PyTorch: 2.4.1+cu121
- Accelerate: 1.0.1
- Datasets: 3.0.0
- Tokenizers: 0.21.4

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

#### MultipleNegativesRankingLoss
```bibtex
@misc{henderson2017efficient,
    title={Efficient Natural Language Response Suggestion for Smart Reply},
    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
    year={2017},
    eprint={1705.00652},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->