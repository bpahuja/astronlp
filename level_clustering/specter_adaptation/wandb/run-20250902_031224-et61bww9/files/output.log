100%|██████████| 2136/2136 [47:24<00:00,  1.33s/it]
{'loss': 3.5813, 'grad_norm': 1.2487928867340088, 'learning_rate': 4.036945812807882e-05, 'epoch': 0.94}
{'loss': 3.0825, 'grad_norm': 1.6994315385818481, 'learning_rate': 2.8054187192118224e-05, 'epoch': 1.87}
{'loss': 3.002, 'grad_norm': 1.3788111209869385, 'learning_rate': 1.5738916256157637e-05, 'epoch': 2.81}
{'loss': 2.9802, 'grad_norm': 1.4059323072433472, 'learning_rate': 3.4236453201970442e-06, 'epoch': 3.75}
{'train_runtime': 2857.8166, 'train_samples_per_second': 47.835, 'train_steps_per_second': 0.747, 'train_loss': 3.149474783336625, 'epoch': 4.0}
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.22it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.78it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.81it/s]
[trial] pos=0.9576 rand=0.7440 gap=0.2136
[cleanup] Freed CUDA cache

[search 2/10] t02_pfeiffer_r16_lr5e-05_m0.5_bs64_ga2
There are adapters available but none are activated for the forward pass.
[warn] gradient_accumulation_steps not supported by your sentence-transformers version. Falling back to no accumulation (increase --batch_size or upgrade ST).
100%|██████████| 2136/2136 [47:10<00:00,  1.33s/it]                  
{'loss': 3.7557, 'grad_norm': 1.1227573156356812, 'learning_rate': 4.039408866995074e-05, 'epoch': 0.94}
{'loss': 3.3731, 'grad_norm': 2.1958560943603516, 'learning_rate': 2.807881773399015e-05, 'epoch': 1.87}
{'loss': 3.2848, 'grad_norm': 1.4018481969833374, 'learning_rate': 1.5788177339901477e-05, 'epoch': 2.81}
{'loss': 3.2439, 'grad_norm': 1.9411745071411133, 'learning_rate': 3.472906403940887e-06, 'epoch': 3.75}
{'train_runtime': 2830.6965, 'train_samples_per_second': 48.293, 'train_steps_per_second': 0.755, 'train_loss': 3.4020809930808533, 'epoch': 4.0}
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.71it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.73it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.73it/s]
[trial] pos=0.9568 rand=0.7850 gap=0.1718
[cleanup] Freed CUDA cache

[search 3/10] t03_houlsby_r8_lr0.0001_m0.0_bs64_ga1
There are adapters available but none are activated for the forward pass.
[warn] gradient_accumulation_steps not supported by your sentence-transformers version. Falling back to no accumulation (increase --batch_size or upgrade ST).
100%|██████████| 2136/2136 [47:09<00:00,  1.32s/it]                  
{'loss': 3.4843, 'grad_norm': 1.0703480243682861, 'learning_rate': 8.528341133645347e-05, 'epoch': 0.94}
{'loss': 2.9873, 'grad_norm': 1.6880090236663818, 'learning_rate': 5.9282371294851804e-05, 'epoch': 1.87}
{'loss': 2.9286, 'grad_norm': 0.923572838306427, 'learning_rate': 3.328133125325013e-05, 'epoch': 2.81}
{'loss': 2.9041, 'grad_norm': 1.0281966924667358, 'learning_rate': 7.2802912116484655e-06, 'epoch': 3.75}
{'train_runtime': 2829.252, 'train_samples_per_second': 48.318, 'train_steps_per_second': 0.755, 'train_loss': 3.0647314121660667, 'epoch': 4.0}
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.77it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.79it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.80it/s]
[trial] pos=0.9637 rand=0.7427 gap=0.2210
[cleanup] Freed CUDA cache

[search 4/10] t04_pfeiffer_r12_lr0.0001_m0.5_bs64_ga1
There are adapters available but none are activated for the forward pass.
[warn] gradient_accumulation_steps not supported by your sentence-transformers version. Falling back to no accumulation (increase --batch_size or upgrade ST).
100%|██████████| 2136/2136 [46:58<00:00,  1.32s/it]                  
{'loss': 3.6467, 'grad_norm': 1.3905599117279053, 'learning_rate': 8.517940717628705e-05, 'epoch': 0.94}
{'loss': 3.2433, 'grad_norm': 2.181955337524414, 'learning_rate': 5.9178367134685386e-05, 'epoch': 1.87}
{'loss': 3.158, 'grad_norm': 2.1832454204559326, 'learning_rate': 3.328133125325013e-05, 'epoch': 2.81}
{'loss': 3.1148, 'grad_norm': 2.391904830932617, 'learning_rate': 7.2802912116484655e-06, 'epoch': 3.75}
{'train_runtime': 2818.504, 'train_samples_per_second': 48.502, 'train_steps_per_second': 0.758, 'train_loss': 3.278759617037541, 'epoch': 4.0}
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.70it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.72it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.71it/s]
[trial] pos=0.9622 rand=0.7665 gap=0.1957
[cleanup] Freed CUDA cache

[search 5/10] t05_pfeiffer_r16_lr5e-05_m0.0_bs64_ga2
There are adapters available but none are activated for the forward pass.
[warn] gradient_accumulation_steps not supported by your sentence-transformers version. Falling back to no accumulation (increase --batch_size or upgrade ST).
100%|██████████| 2136/2136 [46:54<00:00,  1.32s/it]                          
{'loss': 3.5311, 'grad_norm': 1.2863091230392456, 'learning_rate': 4.03448275862069e-05, 'epoch': 0.94}
{'loss': 3.0443, 'grad_norm': 1.4666664600372314, 'learning_rate': 2.802955665024631e-05, 'epoch': 1.87}
{'loss': 2.9868, 'grad_norm': 1.7994582653045654, 'learning_rate': 1.5714285714285715e-05, 'epoch': 2.81}
{'loss': 2.9654, 'grad_norm': 1.604782223701477, 'learning_rate': 3.4236453201970442e-06, 'epoch': 3.75}
{'train_runtime': 2814.9773, 'train_samples_per_second': 48.563, 'train_steps_per_second': 0.759, 'train_loss': 3.1197212304961814, 'epoch': 4.0}
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.69it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.71it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.71it/s]
[trial] pos=0.9560 rand=0.7420 gap=0.2140
[cleanup] Freed CUDA cache

[search 6/10] t06_pfeiffer_r12_lr0.0001_m0.0_bs64_ga2
There are adapters available but none are activated for the forward pass.
[warn] gradient_accumulation_steps not supported by your sentence-transformers version. Falling back to no accumulation (increase --batch_size or upgrade ST).
100%|██████████| 2136/2136 [46:57<00:00,  1.32s/it]                  
{'loss': 3.3348, 'grad_norm': 0.8563050031661987, 'learning_rate': 8.064039408866995e-05, 'epoch': 0.94}
{'loss': 2.9723, 'grad_norm': 2.0916144847869873, 'learning_rate': 5.605911330049262e-05, 'epoch': 1.87}
{'loss': 2.9229, 'grad_norm': 1.903297781944275, 'learning_rate': 3.142857142857143e-05, 'epoch': 2.81}
{'loss': 2.8995, 'grad_norm': 2.303528070449829, 'learning_rate': 6.798029556650247e-06, 'epoch': 3.75}
{'train_runtime': 2817.6939, 'train_samples_per_second': 48.516, 'train_steps_per_second': 0.758, 'train_loss': 3.0229223158475613, 'epoch': 4.0}
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.69it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.72it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.72it/s]
[trial] pos=0.9618 rand=0.7407 gap=0.2211
[cleanup] Freed CUDA cache

[search 7/10] t07_houlsby_r8_lr5e-05_m0.5_bs64_ga1
There are adapters available but none are activated for the forward pass.
[warn] gradient_accumulation_steps not supported by your sentence-transformers version. Falling back to no accumulation (increase --batch_size or upgrade ST).
100%|██████████| 2136/2136 [47:09<00:00,  1.32s/it]                          
{'loss': 3.8129, 'grad_norm': 1.3286337852478027, 'learning_rate': 4.261570462818513e-05, 'epoch': 0.94}
{'loss': 3.4119, 'grad_norm': 1.8772969245910645, 'learning_rate': 2.9615184607384294e-05, 'epoch': 1.87}
{'loss': 3.2786, 'grad_norm': 1.5678905248641968, 'learning_rate': 1.6614664586583466e-05, 'epoch': 2.81}
{'loss': 3.2349, 'grad_norm': 1.5660320520401, 'learning_rate': 3.614144565782632e-06, 'epoch': 3.75}
{'train_runtime': 2829.4277, 'train_samples_per_second': 48.315, 'train_steps_per_second': 0.755, 'train_loss': 3.421268791741646, 'epoch': 4.0}
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.77it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.78it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.78it/s]
[trial] pos=0.9593 rand=0.7921 gap=0.1671
[cleanup] Freed CUDA cache

[search 8/10] t08_houlsby_r12_lr5e-05_m0.5_bs64_ga2
There are adapters available but none are activated for the forward pass.
[warn] gradient_accumulation_steps not supported by your sentence-transformers version. Falling back to no accumulation (increase --batch_size or upgrade ST).
100%|██████████| 2136/2136 [46:39<00:00,  1.31s/it]                  
{'loss': 3.8146, 'grad_norm': 1.5808812379837036, 'learning_rate': 4.036945812807882e-05, 'epoch': 0.94}
{'loss': 3.4485, 'grad_norm': 1.1733338832855225, 'learning_rate': 2.8054187192118224e-05, 'epoch': 1.87}
{'loss': 3.3224, 'grad_norm': 1.527276635169983, 'learning_rate': 1.5738916256157637e-05, 'epoch': 2.81}
{'loss': 3.2737, 'grad_norm': 1.6060950756072998, 'learning_rate': 3.4236453201970442e-06, 'epoch': 3.75}
{'train_runtime': 2799.247, 'train_samples_per_second': 48.836, 'train_steps_per_second': 0.763, 'train_loss': 3.4514310601052274, 'epoch': 4.0}
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.81it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.84it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.85it/s]
[trial] pos=0.9582 rand=0.7965 gap=0.1617
[cleanup] Freed CUDA cache

[search 9/10] t09_pfeiffer_r12_lr5e-05_m0.0_bs64_ga1
There are adapters available but none are activated for the forward pass.
[warn] gradient_accumulation_steps not supported by your sentence-transformers version. Falling back to no accumulation (increase --batch_size or upgrade ST).
100%|██████████| 2136/2136 [47:52<00:00,  1.34s/it]                  
{'loss': 3.5912, 'grad_norm': 1.3563232421875, 'learning_rate': 4.261570462818513e-05, 'epoch': 0.94}
{'loss': 3.0448, 'grad_norm': 2.865873336791992, 'learning_rate': 2.9641185647425902e-05, 'epoch': 1.87}
{'loss': 2.9851, 'grad_norm': 2.2556777000427246, 'learning_rate': 1.6640665626625066e-05, 'epoch': 2.81}
{'loss': 2.9617, 'grad_norm': 2.3547937870025635, 'learning_rate': 3.6401456058242328e-06, 'epoch': 3.75}
{'train_runtime': 2872.4004, 'train_samples_per_second': 47.592, 'train_steps_per_second': 0.744, 'train_loss': 3.1332057424252397, 'epoch': 4.0}
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.70it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.73it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.73it/s]
[trial] pos=0.9547 rand=0.7407 gap=0.2140
[cleanup] Freed CUDA cache

[search 10/10] t10_pfeiffer_r8_lr0.0001_m0.5_bs64_ga2
There are adapters available but none are activated for the forward pass.
[warn] gradient_accumulation_steps not supported by your sentence-transformers version. Falling back to no accumulation (increase --batch_size or upgrade ST).
100%|██████████| 2136/2136 [47:11<00:00,  1.33s/it]                  
{'loss': 3.5343, 'grad_norm': 2.388577699661255, 'learning_rate': 8.06896551724138e-05, 'epoch': 0.94}
{'loss': 3.2052, 'grad_norm': 2.38336181640625, 'learning_rate': 5.610837438423645e-05, 'epoch': 1.87}
{'loss': 3.1277, 'grad_norm': 2.4419734477996826, 'learning_rate': 3.1477832512315274e-05, 'epoch': 2.81}
{'loss': 3.0862, 'grad_norm': 1.9868696928024292, 'learning_rate': 6.8472906403940884e-06, 'epoch': 3.75}
{'train_runtime': 2831.2117, 'train_samples_per_second': 48.285, 'train_steps_per_second': 0.754, 'train_loss': 3.2283039164453857, 'epoch': 4.0}
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.67it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.69it/s]
Batches: 100%|██████████| 10/10 [00:02<00:00,  4.69it/s]
[trial] pos=0.9632 rand=0.7706 gap=0.1926
[cleanup] Freed CUDA cache

=== Hyperparam search (best by gap) ===
{'adapter_type': 'pfeiffer', 'reduction': 12, 'lr': 0.0001, 'mask_prob': 0.0, 'batch_size': 64, 'grad_accum': 2, 'gap': 0.2210676670074463}
