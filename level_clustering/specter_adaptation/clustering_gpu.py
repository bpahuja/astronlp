#!/usr/bin/env python3
"""
Memory-efficient clustering for SPECTER2 embeddings generated by embed_paragraphs.py

Usage:
  python cluster_specter_embeddings.py \
    --embeddings_dir ./embeddings_out \
    --out_dir ./cluster_out \
    --min_cluster_size 30 --target_dims 100 \
    --chunk_size 10000 --use_gpu
"""

import json
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# GPU-accelerated imports
try:
    import cuml
    from cuml.cluster import HDBSCAN as cuHDBSCAN
    from cuml.decomposition import PCA as cuPCA
    from cuml.decomposition import IncrementalPCA as cuIncrementalPCA
    from cuml.cluster import KMeans as cuKMeans
    from cuml.preprocessing import StandardScaler as cuStandardScaler
    import cupy as cp
    import rmm
    GPU_AVAILABLE = True
    print("GPU acceleration enabled with cuML/RAPIDS")
except ImportError as e:
    print(f"GPU libraries not available: {e}")
    print("Falling back to CPU versions...")
    import hdbscan
    from sklearn.decomposition import IncrementalPCA
    from sklearn.cluster import MiniBatchKMeans
    from sklearn.preprocessing import StandardScaler
    GPU_AVAILABLE = False

from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import logging
from typing import Dict, List, Tuple, Optional, Generator
import argparse
from collections import Counter
import pickle
import gc
import psutil
import os
from tqdm import tqdm

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_memory_usage():
    """Monitor memory usage (CPU and GPU)"""
    process = psutil.Process(os.getpid())
    memory_gb = process.memory_info().rss / (1024**3)
    logger.info(f"Current CPU memory usage: {memory_gb:.2f} GB")
    
    if GPU_AVAILABLE:
        try:
            gpu_memory = cp.cuda.runtime.memGetInfo()
            gpu_free_gb = gpu_memory[0] / (1024**3)
            gpu_total_gb = gpu_memory[1] / (1024**3)
            gpu_used_gb = gpu_total_gb - gpu_free_gb
            logger.info(f"GPU memory usage: {gpu_used_gb:.2f}/{gpu_total_gb:.2f} GB")
            return memory_gb, gpu_used_gb
        except Exception as e:
            logger.warning(f"Could not get GPU memory info: {e}")
            return memory_gb, 0
    return memory_gb, 0

def setup_gpu_memory_pool():
    """Setup GPU memory pool for efficient memory management"""
    if GPU_AVAILABLE:
        try:
            rmm.reinitialize(
                pool_allocator=True,
                managed_memory=False,
                initial_pool_size="1GB"
            )
            logger.info("GPU memory pool initialized with 1GB")
        except Exception as e:
            logger.warning(f"Could not initialize GPU memory pool: {e}")

def load_embeddings_from_chunks(embeddings_dir: Path) -> Tuple[np.ndarray, pd.DataFrame]:
    """Load embeddings and metadata from chunk structure created by embed_paragraphs.py"""
    logger.info(f"Loading embeddings from chunk directory: {embeddings_dir}")
    
    # Get embedding info
    info_file = embeddings_dir / "embedding_info.json"
    if not info_file.exists():
        raise FileNotFoundError(f"Embedding info file not found: {info_file}")
    
    with info_file.open() as f:
        embedding_info = json.load(f)
    
    total_paragraphs = embedding_info['total_paragraphs']
    total_chunks = embedding_info['total_chunks']
    embedding_dim = embedding_info.get('embedding_dim', 'unknown')
    
    logger.info(f"Found {total_paragraphs:,} paragraphs in {total_chunks} chunks")
    
    # Get all chunk directories
    chunk_dirs = sorted([d for d in embeddings_dir.iterdir() 
                        if d.is_dir() and d.name.startswith("chunk_")])
    
    if len(chunk_dirs) != total_chunks:
        logger.warning(f"Expected {total_chunks} chunks, found {len(chunk_dirs)}")
    
    # First pass: determine embedding dimension if not in info
    if embedding_dim == 'unknown':
        first_chunk = chunk_dirs[0]
        emb_file = first_chunk / "embeddings.npy"
        sample_emb = np.load(emb_file)
        embedding_dim = sample_emb.shape[1]
        del sample_emb
        logger.info(f"Determined embedding dimension: {embedding_dim}")
    
    # Pre-allocate arrays
    all_embeddings = np.empty((total_paragraphs, embedding_dim), dtype=np.float32)
    all_metadata = []
    
    # Load chunks
    current_idx = 0
    for chunk_dir in tqdm(chunk_dirs, desc="Loading embedding chunks"):
        emb_file = chunk_dir / "embeddings.npy"
        meta_file = chunk_dir / "metadata.csv"
        
        if not (emb_file.exists() and meta_file.exists()):
            logger.warning(f"Missing files in {chunk_dir}")
            continue
        
        # Load embeddings
        chunk_embeddings = np.load(emb_file).astype(np.float32)
        chunk_size = chunk_embeddings.shape[0]
        
        # Store in pre-allocated array
        all_embeddings[current_idx:current_idx + chunk_size] = chunk_embeddings
        current_idx += chunk_size
        
        # Load metadata
        chunk_metadata = pd.read_csv(meta_file)
        all_metadata.append(chunk_metadata)
        
        del chunk_embeddings
        gc.collect()
    
    # Combine metadata
    combined_metadata = pd.concat(all_metadata, ignore_index=True)
    del all_metadata
    gc.collect()
    
    # Verify sizes match
    assert len(all_embeddings) == len(combined_metadata), \
        f"Size mismatch: {len(all_embeddings)} embeddings vs {len(combined_metadata)} metadata entries"
    
    logger.info(f"Successfully loaded {len(all_embeddings):,} embeddings with dimension {embedding_dim}")
    return all_embeddings, combined_metadata

def memory_mapped_array_chunks(embeddings: np.ndarray, chunk_size: int) -> Generator[Tuple[np.ndarray, int, int], None, None]:
    """Generator that yields chunks of an array"""
    total_samples = embeddings.shape[0]
    
    for start_idx in range(0, total_samples, chunk_size):
        end_idx = min(start_idx + chunk_size, total_samples)
        chunk = embeddings[start_idx:end_idx]
        yield chunk, start_idx, end_idx

class AdaptedParagraphClusterer:
    """
    Memory-optimized paragraph clustering adapted for SPECTER2 embeddings.
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.embeddings_dir = Path(config['embeddings_dir'])
        self.embeddings = None
        self.metadata_df = None
        self.cluster_labels = None
        self.clusterer = None
        self.reduced_embeddings = None
        self.chunk_size = config.get('chunk_size', 10000)
        self.gpu_available = GPU_AVAILABLE and config.get('use_gpu', True)
        
        if self.gpu_available:
            setup_gpu_memory_pool()
            logger.info("GPU acceleration enabled")
        else:
            logger.info("Using CPU-only processing")
    
    def load_data(self):
        """Load embeddings and metadata from chunk structure"""
        logger.info("Loading SPECTER2 embeddings and metadata...")
        self.embeddings, self.metadata_df = load_embeddings_from_chunks(self.embeddings_dir)
        
        # Log basic statistics
        logger.info(f"Loaded {len(self.embeddings):,} paragraph embeddings")
        logger.info(f"Embedding dimension: {self.embeddings.shape[1]}")
        logger.info(f"Papers represented: {self.metadata_df['paper_id'].nunique():,}")
        logger.info(f"Average paragraph length: {self.metadata_df['text'].str.len().mean():.1f} characters")
        
        check_memory_usage()
    
    def estimate_memory_requirements(self) -> Dict[str, float]:
        """Estimate memory requirements"""
        if self.embeddings is None:
            return {}
        
        total_samples, embedding_dim = self.embeddings.shape
        bytes_per_element = np.dtype(np.float32).itemsize
        
        estimates = {
            'embeddings_gb': (total_samples * embedding_dim * bytes_per_element) / (1024**3),
            'chunk_gb': (self.chunk_size * embedding_dim * bytes_per_element) / (1024**3),
            'reduced_gb': (total_samples * self.config.get('target_dimensions', 100) * bytes_per_element) / (1024**3),
            'metadata_gb': len(self.metadata_df) * 0.001  # Rough estimate
        }
        
        logger.info(f"Memory estimates (GB): {estimates}")
        return estimates
    
    def optimize_clustering_parameters(self) -> Dict:
        """Optimize parameters using sampling approach"""
        logger.info("Optimizing clustering parameters...")
        
        # Use sample for optimization
        sample_size = min(10000, len(self.embeddings) // 50)
        sample_indices = np.random.choice(len(self.embeddings), sample_size, replace=False)
        sample_embeddings = self.embeddings[sample_indices].copy()
        
        logger.info(f"Using sample of {sample_size:,} points for parameter optimization")
        
        # Quick dimensionality reduction if needed
        if sample_embeddings.shape[1] > 100:
            logger.info("Applying PCA for parameter optimization")
            if self.gpu_available:
                try:
                    sample_gpu = cp.asarray(sample_embeddings, dtype=cp.float32)
                    pca = cuPCA(n_components=min(50, sample_embeddings.shape[1]), random_state=42)
                    sample_embeddings = cp.asnumpy(pca.fit_transform(sample_gpu))
                    del sample_gpu
                    cp.cuda.runtime.deviceSynchronize()
                except Exception as e:
                    logger.warning(f"GPU PCA failed: {e}, using CPU")
                    from sklearn.decomposition import PCA
                    pca = PCA(n_components=min(50, sample_embeddings.shape[1]), random_state=42)
                    sample_embeddings = pca.fit_transform(sample_embeddings).astype(np.float32)
            else:
                from sklearn.decomposition import PCA
                pca = PCA(n_components=min(50, sample_embeddings.shape[1]), random_state=42)
                sample_embeddings = pca.fit_transform(sample_embeddings).astype(np.float32)
        
        # Test parameters
        min_sizes = [20, 30, 50, 80]
        best_score = -1
        best_params = {}
        
        for min_size in min_sizes:
            try:
                if self.gpu_available:
                    sample_gpu = cp.asarray(sample_embeddings, dtype=cp.float32)
                    clusterer = cuHDBSCAN(
                        min_cluster_size=min_size,
                        min_samples=3,
                        cluster_selection_epsilon=0.0,
                        metric='euclidean'
                    )
                    labels = cp.asnumpy(clusterer.fit_predict(sample_gpu))
                    del sample_gpu
                    cp.cuda.runtime.deviceSynchronize()
                else:
                    clusterer = hdbscan.HDBSCAN(
                        min_cluster_size=min_size,
                        metric='euclidean',
                        cluster_selection_method='eom',
                        core_dist_n_jobs=1
                    )
                    labels = clusterer.fit_predict(sample_embeddings)
                
                if len(set(labels)) > 1 and -1 in labels:
                    mask = labels != -1
                    if mask.sum() > min_size * 2:
                        score = silhouette_score(sample_embeddings[mask], labels[mask])
                        
                        num_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                        noise_ratio = (labels == -1).sum() / len(labels)
                        
                        logger.info(f"Min size {min_size}: {num_clusters} clusters, {noise_ratio:.2f} noise, silhouette: {score:.3f}")
                        
                        if score > best_score:
                            best_score = score
                            best_params = {
                                'min_cluster_size': min_size,
                                'silhouette_score': score,
                                'num_clusters': num_clusters,
                                'noise_ratio': noise_ratio
                            }
                
                del clusterer, labels
                gc.collect()
                if self.gpu_available:
                    cp.cuda.runtime.deviceSynchronize()
                            
            except Exception as e:
                logger.warning(f"Error with min_size {min_size}: {e}")
                continue
        
        del sample_embeddings
        gc.collect()
        
        if best_params:
            logger.info(f"Best parameters: {best_params}")
            return best_params
        else:
            logger.warning("Could not find optimal parameters, using default")
            return {'min_cluster_size': self.config.get('min_cluster_size', 30)}
    
    def perform_streaming_dimensionality_reduction(self) -> np.ndarray:
        """Perform dimensionality reduction using streaming approach"""
        target_dims = self.config.get('target_dimensions', 100)
        current_dims = self.embeddings.shape[1]
        
        if current_dims <= target_dims:
            logger.info(f"Embeddings already have {current_dims} dimensions, skipping reduction")
            return self.embeddings
        
        logger.info(f"Performing dimensionality reduction: {current_dims} -> {target_dims}")
        
        if self.gpu_available:
            try:
                return self._gpu_streaming_pca(target_dims)
            except Exception as e:
                logger.warning(f"GPU streaming PCA failed: {e}, falling back to CPU")
        
        return self._cpu_streaming_pca(target_dims)
    
    def _gpu_streaming_pca(self, target_dims: int) -> np.ndarray:
        """GPU streaming PCA implementation"""
        batch_size = min(self.chunk_size, 8000)
        total_samples = len(self.embeddings)
        
        ipca = cuIncrementalPCA(n_components=target_dims, batch_size=batch_size)
        
        # Fit phase
        logger.info("Fitting GPU incremental PCA...")
        processed_samples = 0
        
        for chunk, start_idx, end_idx in memory_mapped_array_chunks(self.embeddings, batch_size):
            chunk_gpu = cp.asarray(chunk, dtype=cp.float32)
            ipca.partial_fit(chunk_gpu)
            processed_samples = end_idx
            
            if processed_samples % (batch_size * 5) == 0:
                logger.info(f"PCA fitting progress: {processed_samples:,}/{total_samples:,}")
            
            del chunk_gpu
            cp.cuda.runtime.deviceSynchronize()
            gc.collect()
        
        # Transform phase
        logger.info("Transforming with GPU PCA...")
        reduced_embeddings = np.empty((total_samples, target_dims), dtype=np.float32)
        
        processed_samples = 0
        for chunk, start_idx, end_idx in memory_mapped_array_chunks(self.embeddings, batch_size):
            chunk_gpu = cp.asarray(chunk, dtype=cp.float32)
            transformed_gpu = ipca.transform(chunk_gpu)
            reduced_embeddings[start_idx:end_idx] = cp.asnumpy(transformed_gpu)
            processed_samples = end_idx
            
            if processed_samples % (batch_size * 5) == 0:
                logger.info(f"PCA transform progress: {processed_samples:,}/{total_samples:,}")
            
            del chunk_gpu, transformed_gpu
            cp.cuda.runtime.deviceSynchronize()
            gc.collect()
        
        logger.info("GPU streaming PCA complete")
        return reduced_embeddings
    
    def _cpu_streaming_pca(self, target_dims: int) -> np.ndarray:
        """CPU streaming PCA implementation"""
        batch_size = min(self.chunk_size, 5000)
        total_samples = len(self.embeddings)
        
        ipca = IncrementalPCA(n_components=target_dims, batch_size=batch_size)
        
        # Fit phase
        logger.info("Fitting CPU incremental PCA...")
        processed_samples = 0
        
        for chunk, start_idx, end_idx in memory_mapped_array_chunks(self.embeddings, batch_size):
            ipca.partial_fit(chunk)
            processed_samples = end_idx
            
            if processed_samples % (batch_size * 10) == 0:
                logger.info(f"PCA fitting progress: {processed_samples:,}/{total_samples:,}")
            
            gc.collect()
        
        # Transform phase
        logger.info("Transforming with CPU PCA...")
        reduced_embeddings = np.empty((total_samples, target_dims), dtype=np.float32)
        
        processed_samples = 0
        for chunk, start_idx, end_idx in memory_mapped_array_chunks(self.embeddings, batch_size):
            transformed_chunk = ipca.transform(chunk).astype(np.float32)
            reduced_embeddings[start_idx:end_idx] = transformed_chunk
            processed_samples = end_idx
            
            if processed_samples % (batch_size * 10) == 0:
                logger.info(f"PCA transform progress: {processed_samples:,}/{total_samples:,}")
            
            del transformed_chunk
            gc.collect()
        
        logger.info(f"CPU streaming PCA complete. Explained variance ratio: {ipca.explained_variance_ratio_.sum():.3f}")
        return reduced_embeddings
    
    def perform_clustering(self, embeddings: np.ndarray, min_cluster_size: int) -> np.ndarray:
        """Perform clustering on embeddings"""
        logger.info(f"Starting clustering with {len(embeddings):,} embeddings...")
        
        # Check if we can fit embeddings in memory for direct clustering
        embedding_size_gb = embeddings.nbytes / (1024**3)
        available_memory_gb = psutil.virtual_memory().available / (1024**3)
        
        logger.info(f"Embeddings size: {embedding_size_gb:.2f} GB, Available memory: {available_memory_gb:.2f} GB")
        
        if embedding_size_gb * 2.0 < available_memory_gb:  # Conservative estimate
            return self._direct_clustering(embeddings, min_cluster_size)
        else:
            logger.info("Using chunked clustering approach")
            return self._chunked_clustering(embeddings, min_cluster_size)
    
    def _direct_clustering(self, embeddings: np.ndarray, min_cluster_size: int) -> np.ndarray:
        """Direct clustering when we have enough memory"""
        try:
            if self.gpu_available:
                logger.info("Attempting GPU HDBSCAN clustering...")
                
                # Check GPU memory
                gpu_memory = cp.cuda.runtime.memGetInfo()
                gpu_free_gb = gpu_memory[0] / (1024**3)
                embedding_size_gb = embeddings.nbytes / (1024**3)
                
                if embedding_size_gb * 2.5 < gpu_free_gb:
                    embeddings_gpu = cp.asarray(embeddings, dtype=cp.float32)
                    
                    self.clusterer = cuHDBSCAN(
                        min_cluster_size=min_cluster_size,
                        min_samples=self.config.get('min_samples', 3),
                        cluster_selection_epsilon=0.0,
                        metric='euclidean'
                    )
                    
                    cluster_labels_gpu = self.clusterer.fit_predict(embeddings_gpu)
                    cluster_labels = cp.asnumpy(cluster_labels_gpu)
                    
                    del embeddings_gpu, cluster_labels_gpu
                    cp.cuda.runtime.deviceSynchronize()
                    
                    num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
                    noise_points = (cluster_labels == -1).sum()
                    logger.info(f"GPU HDBSCAN complete: {num_clusters} clusters, {noise_points:,} noise points")
                    
                    return cluster_labels
                else:
                    raise MemoryError("Insufficient GPU memory")
            else:
                raise ImportError("GPU not available")
                
        except (MemoryError, ImportError, Exception) as e:
            logger.warning(f"GPU clustering failed: {e}, using CPU HDBSCAN")
            
            try:
                self.clusterer = hdbscan.HDBSCAN(
                    min_cluster_size=min_cluster_size,
                    min_samples=self.config.get('min_samples', 3),
                    metric='euclidean',
                    cluster_selection_method='eom',
                    core_dist_n_jobs=1
                )
                cluster_labels = self.clusterer.fit_predict(embeddings)
                
                num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
                noise_points = (cluster_labels == -1).sum()
                logger.info(f"CPU HDBSCAN complete: {num_clusters} clusters, {noise_points:,} noise points")
                
                return cluster_labels
                
            except MemoryError:
                logger.warning("CPU HDBSCAN failed, falling back to chunked K-Means")
                return self._chunked_clustering(embeddings, min_cluster_size)
    
    def _chunked_clustering(self, embeddings: np.ndarray, min_cluster_size: int) -> np.ndarray:
        """Fallback chunked clustering using K-Means"""
        logger.info("Using chunked K-Means clustering...")
        
        total_samples = len(embeddings)
        n_clusters = min(500, max(10, total_samples // (min_cluster_size * 3)))
        
        logger.info(f"Using {n_clusters} clusters for {total_samples:,} samples")
        
        if self.gpu_available:
            try:
                return self._gpu_chunked_kmeans(embeddings, n_clusters)
            except Exception as e:
                logger.warning(f"GPU K-Means failed: {e}, using CPU")
        
        # CPU MiniBatch K-Means
        batch_size = min(self.chunk_size, 5000)
        
        kmeans = MiniBatchKMeans(
            n_clusters=n_clusters,
            batch_size=batch_size,
            random_state=42,
            n_init=3,
            max_iter=100
        )
        
        # Fit incrementally
        logger.info("Fitting MiniBatch K-Means...")
        processed_samples = 0
        
        for chunk, start_idx, end_idx in memory_mapped_array_chunks(embeddings, batch_size):
            kmeans.partial_fit(chunk)
            processed_samples = end_idx
            
            if processed_samples % (batch_size * 20) == 0:
                logger.info(f"K-Means fitting progress: {processed_samples:,}/{total_samples:,}")
            
            gc.collect()
        
        # Predict labels
        logger.info("Predicting cluster labels...")
        cluster_labels = kmeans.predict(embeddings)
        
        self.clusterer = kmeans
        logger.info(f"K-Means complete: {n_clusters} clusters")
        
        return cluster_labels
    
    def _gpu_chunked_kmeans(self, embeddings: np.ndarray, n_clusters: int) -> np.ndarray:
        """GPU chunked K-Means implementation"""
        logger.info("Using GPU K-Means...")
        
        # Sample for initial fitting
        sample_size = min(50000, len(embeddings))
        sample_indices = np.random.choice(len(embeddings), sample_size, replace=False)
        sample_embeddings = embeddings[sample_indices]
        
        sample_gpu = cp.asarray(sample_embeddings, dtype=cp.float32)
        kmeans = cuKMeans(n_clusters=n_clusters, random_state=42, n_init=3, max_iter=100)
        kmeans.fit(sample_gpu)
        
        del sample_gpu, sample_embeddings
        cp.cuda.runtime.deviceSynchronize()
        gc.collect()
        
        # Predict on full dataset
        embeddings_gpu = cp.asarray(embeddings, dtype=cp.float32)
        cluster_labels = cp.asnumpy(kmeans.predict(embeddings_gpu))
        
        del embeddings_gpu
        cp.cuda.runtime.deviceSynchronize()
        
        self.clusterer = kmeans
        logger.info(f"GPU K-Means complete: {n_clusters} clusters")
        
        return cluster_labels
    
    def extract_top_terms_per_cluster(self, cluster_labels: np.ndarray, n_top: int = 12) -> Dict[int, List[str]]:
        """Extract top terms for each cluster using TF-IDF"""
        logger.info("Extracting top terms per cluster...")
        
        # Get texts from metadata
        texts = self.metadata_df['text'].tolist()
        
        # Filter out noise points
        mask = cluster_labels >= 0
        if not mask.any():
            return {}
        
        filtered_texts = [t for t, keep in zip(texts, mask) if keep]
        filtered_labels = cluster_labels[mask]
        
        # Subsample if too many texts
        max_texts = 100000
        if len(filtered_texts) > max_texts:
            logger.info(f"Subsampling {max_texts} texts for TF-IDF")
            indices = np.random.choice(len(filtered_texts), max_texts, replace=False)
            filtered_texts = [filtered_texts[i] for i in indices]
            filtered_labels = filtered_labels[indices]
        
        # Compute TF-IDF
        vectorizer = TfidfVectorizer(
            ngram_range=(1, 2),
            max_features=50000,
            min_df=3,
            max_df=0.95,
            stop_words='english'
        )
        
        X = vectorizer.fit_transform(filtered_texts)
        X = normalize(X, norm="l2")
        vocab = np.array(vectorizer.get_feature_names_out())
        
        # Extract top terms per cluster
        top_terms = {}
        unique_labels = np.unique(filtered_labels)
        
        for cluster_id in tqdm(unique_labels, desc="Extracting terms"):
            mask_cluster = filtered_labels == cluster_id
            if not mask_cluster.any():
                continue
            
            # Compute mean TF-IDF scores for this cluster
            cluster_tfidf = X[mask_cluster].mean(axis=0).A1
            
            # Get top terms
            top_indices = cluster_tfidf.argsort()[-n_top:][::-1]
            top_terms[int(cluster_id)] = vocab[top_indices].tolist()
        
        logger.info(f"Extracted top terms for {len(top_terms)} clusters")
        return top_terms
    
    def save_results(self, cluster_labels: np.ndarray, top_terms: Dict[int, List[str]]):
        """Save clustering results"""
        out_dir = Path(self.config['out_dir'])
        out_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Saving results to {out_dir}")
        
        # Save detailed results
        output_df = self.metadata_df.copy()
        output_df['cluster'] = cluster_labels
        
        # Save in chunks to avoid memory issues
        chunk_size = 100000
        for i in range(0, len(output_df), chunk_size):
            chunk_df = output_df.iloc[i:i+chunk_size]
            chunk_file = out_dir / f"paragraph_clusters_part_{i//chunk_size:04d}.csv"
            chunk_df.to_csv(chunk_file, index=False)
            logger.info(f"Saved chunk {i//chunk_size}")
        
        # Save summary
        n = len(cluster_labels)
        n_noise = (cluster_labels == -1).sum()
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        noise_pct = 100.0 * n_noise / n
        
        summary = {
            "paragraphs": int(n),
            "clusters": int(n_clusters),
            "noise": int(n_noise),
            "noise_pct": float(noise_pct),
            "top_terms": top_terms,
            "clustering_params": {
                "min_cluster_size": self.config.get('min_cluster_size', 30),
                "target_dimensions": self.config.get('target_dimensions', 100),
                "chunk_size": self.chunk_size,
                "gpu_used": self.gpu_available
            }
        }
        
        with (out_dir / "cluster_summary.json").open("w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # Paper-level bag-of-clusters
        logger.info("Computing paper-level bag-of-clusters...")
        clustered_df = output_df[output_df['cluster'] >= 0]
        if not clustered_df.empty:
            bag_of_methods = (clustered_df
                            .groupby(['paper_id', 'cluster']).size()
                            .unstack(fill_value=0)
                            .sort_index(axis=1))
            bag_of_methods.to_csv(out_dir / "paper_bag_of_methods.csv")
            logger.info("Saved paper bag-of-methods")
        
        logger.info(f"Results saved successfully!")
        logger.info(f"Found {n_clusters} clusters with {noise_pct:.1f}% noise points")
    
    def run_clustering_pipeline(self):
        """Run the complete clustering pipeline"""
        logger.info("Starting SPECTER2 embedding clustering pipeline...")
        
        try:
            # Load data
            self.load_data()
            check_memory_usage()
            
            # Optimize parameters if requested
            if not self.config.get('skip_optimization', False):
                optimal_params = self.optimize_clustering_parameters()
                min_cluster_size = optimal_params.get('min_cluster_size', self.config.get('min_cluster_size', 30))
            else:
                min_cluster_size = self.config.get('min_cluster_size', 30)
            
            # Dimensionality reduction if needed
            embeddings = self.embeddings
            if self.embeddings.shape[1] > self.config.get('max_dimensions_direct', 150):
                logger.info("Performing dimensionality reduction...")
                self.reduced_embeddings = self.perform_streaming_dimensionality_reduction()
                embeddings = self.reduced_embeddings
                # Free original embeddings to save memory
                del self.embeddings
                self.embeddings = None
                gc.collect()
            
            check_memory_usage()
            
            # Perform clustering
            logger.info(f"Starting clustering with min_cluster_size={min_cluster_size}")
            self.cluster_labels = self.perform_clustering(embeddings, min_cluster_size)
            
            # Extract top terms
            top_terms = self.extract_top_terms_per_cluster(self.cluster_labels)
            
            # Save results
            self.save_results(self.cluster_labels, top_terms)
            
            # Final cleanup
            if self.reduced_embeddings is not None:
                del self.reduced_embeddings
            gc.collect()
            if self.gpu_available:
                cp.cuda.runtime.deviceSynchronize()
            
            logger.info("Clustering pipeline completed successfully!")
            
            return {
                'cluster_labels': self.cluster_labels,
                'top_terms': top_terms,
                'n_clusters': len(set(self.cluster_labels)) - (1 if -1 in self.cluster_labels else 0),
                'noise_points': (self.cluster_labels == -1).sum()
            }
            
        except Exception as e:
            logger.error(f"Clustering pipeline failed: {e}")
            raise


def main():
    """Main function for clustering SPECTER2 embeddings"""
    parser = argparse.ArgumentParser(description='Memory-Efficient Clustering for SPECTER2 Embeddings')
    parser.add_argument('--embeddings_dir', required=True, help='Directory with embedding chunks from embed_paragraphs.py')
    parser.add_argument('--out_dir', required=True, help='Output directory for clustering results')
    
    # Clustering parameters
    parser.add_argument('--min_cluster_size', type=int, default=30, help='Minimum cluster size')
    parser.add_argument('--min_samples', type=int, default=3, help='Minimum samples parameter')
    parser.add_argument('--target_dimensions', type=int, default=100, help='Target dimensions after PCA')
    parser.add_argument('--max_dimensions_direct', type=int, default=150, 
                       help='Max dimensions for direct clustering (no PCA)')
    
    # Memory optimization
    parser.add_argument('--chunk_size', type=int, default=10000, help='Chunk size for processing')
    parser.add_argument('--use_gpu', action='store_true', help='Enable GPU acceleration')
    parser.add_argument('--skip_optimization', action='store_true', help='Skip parameter optimization')
    
    # System check
    parser.add_argument('--check_system', action='store_true', help='Check system requirements')
    
    args = parser.parse_args()
    
    if args.check_system:
        check_system_requirements()
        return
    
    # Verify input directory exists
    embeddings_dir = Path(args.embeddings_dir)
    if not embeddings_dir.exists():
        logger.error(f"Embeddings directory not found: {embeddings_dir}")
        return
    
    # Check for required files
    info_file = embeddings_dir / "embedding_info.json"
    if not info_file.exists():
        logger.error(f"Embedding info file not found: {info_file}")
        return
    
    # Create config
    config = {
        'embeddings_dir': str(embeddings_dir),
        'out_dir': args.out_dir,
        'min_cluster_size': args.min_cluster_size,
        'min_samples': args.min_samples,
        'target_dimensions': args.target_dimensions,
        'max_dimensions_direct': args.max_dimensions_direct,
        'chunk_size': args.chunk_size,
        'use_gpu': args.use_gpu,
        'skip_optimization': args.skip_optimization
    }
    
    # Log configuration
    logger.info("Configuration:")
    for key, value in config.items():
        logger.info(f"  {key}: {value}")
    
    # Run clustering
    try:
        clusterer = AdaptedParagraphClusterer(config)
        results = clusterer.run_clustering_pipeline()
        
        logger.info(f"Clustering completed successfully!")
        logger.info(f"Found {results['n_clusters']} clusters")
        logger.info(f"Noise points: {results['noise_points']:,}")
        
    except Exception as e:
        logger.error(f"Clustering failed: {e}")
        raise


def check_system_requirements():
    """Check system requirements and provide recommendations"""
    logger.info("Checking system requirements...")
    
    # Check RAM
    memory_info = psutil.virtual_memory()
    total_ram_gb = memory_info.total / (1024**3)
    available_ram_gb = memory_info.available / (1024**3)
    
    logger.info(f"Total RAM: {total_ram_gb:.1f} GB")
    logger.info(f"Available RAM: {available_ram_gb:.1f} GB")
    
    if available_ram_gb < 8:
        logger.warning("Less than 8 GB RAM available. Consider closing other applications.")
    
    # Check disk space
    disk_usage = psutil.disk_usage('.')
    free_disk_gb = disk_usage.free / (1024**3)
    logger.info(f"Available disk space: {free_disk_gb:.1f} GB")
    
    if free_disk_gb < 10:
        logger.warning("Less than 10 GB disk space available.")
    
    # Check GPU
    if GPU_AVAILABLE:
        try:
            gpu_count = cp.cuda.runtime.getDeviceCount()
            for i in range(gpu_count):
                props = cp.cuda.runtime.getDeviceProperties(i)
                name = props['name'].decode('utf-8')
                memory_gb = props['totalGlobalMem'] / (1024**3)
                logger.info(f"GPU {i}: {name} ({memory_gb:.1f} GB)")
        except Exception as e:
            logger.warning(f"GPU info unavailable: {e}")
    else:
        logger.info("No GPU acceleration available - will use CPU clustering")


if __name__ == "__main__":
    main()