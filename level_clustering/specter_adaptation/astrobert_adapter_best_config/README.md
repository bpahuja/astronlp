---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- dense
- generated_from_trainer
- dataset_size:34208
- loss:MultipleNegativesRankingLoss
base_model: adsabs/astroBERT
widget:
- source_sentence: '##gn 347. 3043, [ MATH _ tex = - ] 0. 0689 Align 29. 2 Align â€“
    12671 Align RX J1713. 7 [ MATH _ tex = - - ] 3946 \ ac nw Align 2011 - 07 - 01
    Align 17 : 11 : 47. 5, [ MATH _ tex = - ] 39 : 33 : 41. 2 Align 347. 2959, [ MATH
    _ tex = - ] 0. 0807 Align 89. 9 Align â€“ 6721 Align CTB 37A Align 2006 - 10 - 07
    Align 17 : 14 : 35. 8, [ MATH _ tex = - ] 38 : 31 : 24. 6 Align 348. 4558, [ MATH
    _ tex = + ] 0. 087'
  sentences:
  - 'In particular , we focus on the case of reconstructing polarized signal emitted
    in the radio and sub-mm regimes : i ) at [MATH_tex=nulesssimSI60GHz] where the
    emission is mostly dominated by Galactic synchrotron , described by a power law
    [MATH_tex=beta_synchsim-3] \citep 2018A Align Aâ€¦618A.166K , ii ) at [MATH_tex=nugtrsimSI150GHz]
    where most of the polarization is due to the thermal Galactic dust grains aligning
    with the Galactic magnetic field , described by a modified blackbody law \citep
    planck2018 . In this study , we mainly focus on inpainting maps of polarized emissions
    in two microwave regimes : namely synchrotron and thermal dust .'
  - More specifically , we assume an NFW density profile , a general Schechter luminosity
    function and a Gaussian model for BCGs to model clusters , and use the spectroscopic
    measurements and obtained error distributions of galaxy photometric redshifts
    from the Sloan Digital Sky Survey to incorporate redshift uncertainties .
  - However , its contribution to the entire background is tiny . The ME spectra of
    the high Galactic latitude blank sky observations measured with the small FOVs
    in three COR regions are shown in Figure 8 .
- source_sentence: For the cubic expansion on [MATH_tex=H(z)] without a CMB prior
    on [MATH_tex=r_d] , we obtain For the polynomial expansion on [MATH_tex=H(z)]
    without a CMB prior on [MATH_tex=r_d] , we obtain We can not provide a strict
    constraint on the Hubble constant in these two model-independent reconstructions
    , and the results are consistent with both Planck 2018 and SH0ES 2019 .
  sentences:
  - ( 26 ) [MATH_tex=f_gas] sample and different distance indicators ( SN Ia observations
    , SZ effect/X-ray measurements of galaxy clusters , and Planck â€™ s best-fitted
    [MATH_tex=Lambda] CDM cosmology ) .
  - The posterior probability on H [MATH_tex=_0] from [MATH_tex=N] gravitational-wave
    ( GW ) events can be computed as where [MATH_tex=x_textGW] is the set of GW data
    and [MATH_tex=D_textGW] indicates that the detection was made in the form of a
    GW .
  - 'After inspecting the light curves and [MATH_tex=I] -band images of these stars
    as well , a final combined sample of 49 stars remained , of which 39 were actually
    monitored . The basic properties of the sample are listed in Table 1 in the first
    columns : Identification , R.A. , Declination , pulsation period , galactic coordinates
    . The observations were carried out with the SOFI infra-red camera on the 3.5m
    NTT on ESO/La Silla in the nights of 2007 , June 24 , 28 , July 3 , 8 in visitor
    mode . Photometric conditions were excellent on the second and third night with
    seeing as low as 0.6â€³ and the telescope was actually defocused .'
- source_sentence: The most likely scenario is that the synchrotron emitting electrons
    are ( re ) accelerated by outward moving shocks to a Lorentz factor of [MATH_tex=gamma_esim
    10^3-10^5] ( e.g . ) . The cluster was observed with the Westerbork Radio Synthesis
    Telescope ( WSRT ) at 21 cm ( see Sec . 2.1 ) and subsequently with the Giant
    Meterwave Radio Telescope ( GMRT ) at three separate frequencies to enable spectral
    analysis ( see Sec . 2.2 ) .
  sentences:
  - We only briefly summarize methods where this is possible and discuss the MAGMA2
    choices more extensively where needed . For a general introduction to the Smooth
    Particle Hydrodynamics ( SPH ) method , we refer to review articles ( ) , a detailed
    step-by-step derivation of both Newtonian and relativistic SPH can be found in
    ( ) .
  - Briefly , we use the SExtractor shape parameters ( [MATH_tex=C_xx] , [MATH_tex=C_xy]
    , and [MATH_tex=C_yy] ) to define an ellipse for each object in the field , and
    we identify the host as the object that includes the supernova position inside
    its ellipse with the smallest possible scaling unless the scaling factor , [MATH_tex=R]
    , is greater than 5 .
  - A summary of the main modifications of the imager setup is presented in table
    5 . SNLS exploits the DEEP component of the CFHT Legacy Survey ( CFHTLS ) , which
    targets four low Galactic extinction fields ( see table 1 for coordinates and
    extinction ) .
- source_sentence: We briefly review the core components of the galaxy formation model
    currently implemented in the moving-mesh code arepo ( ) , which have been detailed
    in prior work ( ) and used for various cosmological studies ( ) , and then describe
    the new dust physics we have added . arepo uses a dynamic Voronoi tessellation
    to solve the equations of ideal hydrodynamics with a finite-volume method .
  sentences:
  - 'The MHD equations in vector form are : and where [MATH_tex=rho] denotes the gas
    density , [MATH_tex=V] velocity vector , [MATH_tex=p] pressure , [MATH_tex=B]
    magnetic field , [MATH_tex=c] the speed of light , [MATH_tex=E] electric field
    , and [MATH_tex=T] temperature , while [MATH_tex=U] is the internal energy per
    unit mass , [MATH_tex=I] the unit tensor , [MATH_tex=k_rm B] the Boltzmann constant
    , [MATH_tex=m(=rm const.)] the mean molecular mass , and [MATH_tex=g] the uniform
    gravitational acceleration .'
  - If we , at the first step of combining PAO and TA spectra , limit the fitting
    procedure to the energies below [MATH_tex=2times 10^19] using not all points measured
    by PAO and TA , the fallowing four parameters of the Galactic component in Eq
    .
  - Having obtained a fitting formula for the non-linear matter power spectrum in
    the DDM model , we now compare the model predictions to the data on weak lensing
    , CMB and BAO , using a modified CosmoMC code [ <CIT> ] .
- source_sentence: 'For our self - consistency test of GR, wherever needed, we assume
    a flat [ MATH _ tex = Lambda ] CDM fiducial cosmology with Planck 2018 CMB [ MATH
    _ tex = + ] BAO parameters \ citep Planck2018 - cosmo : [ MATH _ tex = Omega _
    rm m = 0. 3111pm 0. 0056 ], [ MATH _ tex = Omega _ rm ch ^ 2 = 0. 11933pm 0. 00091
    ], [ MATH _ tex = Omega _ rm bh ^ 2 = 0. 02242pm 0. 00014 ], [ MATH _ tex = n
    _ rm s = 0. 9665pm 0. 0038 ], [ MATH _ tex = H _ 0 = 67. 66pm 0. 42 ], and [ MATH
    _ tex = sigma _ 8 = 0. 810'
  sentences:
  - We run a series of Markov Chains Monte Carlo using the publicly available codes
    CAMB and CosmoMC <CIT> , properly modified to correctly account for the physics
    described in the previous section ( influence of quintessence and its perturbations
    ) .
  - The MRC bolometric luminosity varies over [MATH_tex=sim] 3 orders of magnitude
    in the sample , [MATH_tex=4.3times,10^2,L_sun<] MRC [MATH_tex=<,3.9times,10^5,L_sun]
    ( Fig . 4 ) with a mean luminosity of [MATH_tex=4.8times,10^4,L_sun] ( median
    of [MATH_tex=3.1times,10^4,L_sun] ) .
  - Hence , as shown in Figure 8 , the estimated [MATH_tex=C_ell] recover the input
    power spectrum better even at large [MATH_tex=ell] ( [MATH_tex=geq 6times 10^3]
    ) clearly demonstrating the need of correct spectral modelling of the point sources
    . The left panel of Figure 9 shows the angular power spectra [MATH_tex=C_ell]
    estimated using the residual visibility data obtained from Run ( b ) , ( c ) and
    ( d ) for [MATH_tex=it nterms=2] but different CLEANing threshold .
pipeline_tag: sentence-similarity
library_name: sentence-transformers
---

# SentenceTransformer based on adsabs/astroBERT

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [adsabs/astroBERT](https://huggingface.co/adsabs/astroBERT). It maps sentences & paragraphs to a 768-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [adsabs/astroBERT](https://huggingface.co/adsabs/astroBERT) <!-- at revision b692b96fe7c386c096646f52ff2d9b52f995952e -->
- **Maximum Sequence Length:** 256 tokens
- **Output Dimensionality:** 768 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ðŸ¤— Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    'For our self - consistency test of GR, wherever needed, we assume a flat [ MATH _ tex = Lambda ] CDM fiducial cosmology with Planck 2018 CMB [ MATH _ tex = + ] BAO parameters \\ citep Planck2018 - cosmo : [ MATH _ tex = Omega _ rm m = 0. 3111pm 0. 0056 ], [ MATH _ tex = Omega _ rm ch ^ 2 = 0. 11933pm 0. 00091 ], [ MATH _ tex = Omega _ rm bh ^ 2 = 0. 02242pm 0. 00014 ], [ MATH _ tex = n _ rm s = 0. 9665pm 0. 0038 ], [ MATH _ tex = H _ 0 = 67. 66pm 0. 42 ], and [ MATH _ tex = sigma _ 8 = 0. 810',
    'We run a series of Markov Chains Monte Carlo using the publicly available codes CAMB and CosmoMC <CIT> , properly modified to correctly account for the physics described in the previous section ( influence of quintessence and its perturbations ) .',
    'The MRC bolometric luminosity varies over [MATH_tex=sim] 3 orders of magnitude in the sample , [MATH_tex=4.3times,10^2,L_sun<] MRC [MATH_tex=<,3.9times,10^5,L_sun] ( Fig . 4 ) with a mean luminosity of [MATH_tex=4.8times,10^4,L_sun] ( median of [MATH_tex=3.1times,10^4,L_sun] ) .',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 768]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# tensor([[1.0000, 0.9727, 0.6686],
#         [0.9727, 1.0000, 0.6097],
#         [0.6686, 0.6097, 1.0000]])
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset

* Size: 34,208 training samples
* Columns: <code>sentence_0</code> and <code>sentence_1</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                          | sentence_1                                                                          |
  |:--------|:------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------|
  | type    | string                                                                              | string                                                                              |
  | details | <ul><li>min: 32 tokens</li><li>mean: 87.33 tokens</li><li>max: 184 tokens</li></ul> | <ul><li>min: 34 tokens</li><li>mean: 87.85 tokens</li><li>max: 184 tokens</li></ul> |
* Samples:
  | sentence_0                                                                                                                                                                                                                                                                                                                                                                                                                                                         | sentence_1                                                                                                                                                                                                                                                                                                                                                                     |
  |:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | <code>Due to operational constraints , these observations could not coincide with the XMM-Newton/INTEGRAL monitoring . Before our XMM-Newton/INTEGRAL observations , and in between the XMM-Newton and Chandra/HST observations , we monitored Mrk 509 with Swift , using both its X-ray telescope ( XRT ) and the UltraViolet and Optical Telescope ( UVOT ) .</code>                                                                                             | <code>Briefly , we use the SExtractor shape parameters ( [MATH_tex=C_xx] , [MATH_tex=C_xy] , and [MATH_tex=C_yy] ) to define an ellipse for each object in the field , and we identify the host as the object that includes the supernova position inside its ellipse with the smallest possible scaling unless the scaling factor , [MATH_tex=R] , is greater than 5 .</code> |
  | <code>We have to point out that also in this case we are using a diagonal covariance matrix , because it is not possible to forecast out-of-diagonal terms ; this can lead to underestimated errors on cosmological parameters .</code>                                                                                                                                                                                                                            | <code>More interestingly , benefit from the redshift coverage of background sources in the lensing systems , the methodology proposed in this analysis may provide improved constraints on the DGP model , which was ruled out observationally considering the precision cosmological observational data .</code>                                                              |
  | <code>( 20 ) , the stellar component contribution at the wavelength of 4600Ã… and the aperture used at their observations is approximately equal to [MATH_tex=2.2times 10^-14mbox erg cm s AA ] . As it is known from observations ( see , e.g. , Crenshaw et al . 11 ; Peterson et al . 43 ) , the light curves of an active galactic nucleus in the optical and UV continua can be rather different , and the relation between the continua is nonlinear .</code> | <code>We used the ESO Very Large Telescope and Focal Reducer/Low Dispersion Spectrograph # 2 ( FORS2 ) in multi-object spectroscopy ( MOS ) mode on 12 Jan 2007 using three 800 s exposures with the 300V grism , centred at [MATH_tex=lambda] =5900Ã… .</code>                                                                                                                 |
* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:
  ```json
  {
      "scale": 20.0,
      "similarity_fct": "cos_sim",
      "gather_across_devices": false
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `per_device_train_batch_size`: 32
- `per_device_eval_batch_size`: 32
- `num_train_epochs`: 8
- `fp16`: True
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 32
- `per_device_eval_batch_size`: 32
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 8
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: True
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `tp_size`: 0
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: None
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: False
- `prompts`: None
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin
- `router_mapping`: {}
- `learning_rate_mapping`: {}

</details>

### Training Logs
| Epoch  | Step | Training Loss |
|:------:|:----:|:-------------:|
| 0.4677 | 500  | 2.9285        |
| 0.9355 | 1000 | 2.3798        |
| 1.4032 | 1500 | 2.31          |
| 1.8709 | 2000 | 2.2899        |
| 2.3386 | 2500 | 2.2605        |
| 2.8064 | 3000 | 2.2544        |
| 3.2741 | 3500 | 2.2242        |
| 3.7418 | 4000 | 2.2214        |
| 4.2095 | 4500 | 2.2072        |
| 4.6773 | 5000 | 2.2079        |
| 5.1450 | 5500 | 2.19          |
| 5.6127 | 6000 | 2.1887        |
| 6.0804 | 6500 | 2.1792        |
| 6.5482 | 7000 | 2.1744        |
| 7.0159 | 7500 | 2.1777        |
| 7.4836 | 8000 | 2.1569        |
| 7.9514 | 8500 | 2.1664        |


### Framework Versions
- Python: 3.12.3
- Sentence Transformers: 5.1.0
- Transformers: 4.51.3
- PyTorch: 2.4.1+cu121
- Accelerate: 1.0.1
- Datasets: 3.0.0
- Tokenizers: 0.21.4

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

#### MultipleNegativesRankingLoss
```bibtex
@misc{henderson2017efficient,
    title={Efficient Natural Language Response Suggestion for Smart Reply},
    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
    year={2017},
    eprint={1705.00652},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->